library(quanteda)
library(stopwords)
library(tibble)
library(dplyr)
stop_w <- tibble(word = stopwords(source = "stopwords-iso", language = "pt"))
knitr::opts_chunk$set(echo = TRUE,message = F,warning = F)
# Baixando e carregando o pacote
library( rvest )
library( xml2 )
library(shiny)
library(httr)
library(httr2)
# Link para:
url <-  " https://jornaldaparaiba.com.br/ "
url
# Ler o HTML da página da web
url <- c("https://www.jornaldaparaiba.com.br")
pagina <- read_html(url)
elementos <- pagina %>%
html_nodes("div.container a")%>%
html_text2()
# Carregar o pacote rvest
library(rvest)
library(xml2)
library(shiny)
library(httr)
library(httr2)
# Ler o HTML da página da web
url <- "https://www.jornaldaparaiba.com.br"
pagina <- read_html(url)
dt <- html_nodes(pagina, "div.container a")
# Exibir o conteúdo dos elementos selecionados
cat("Conteúdo dos elementos selecionados:\n")
cat(html_text(dt), sep = "\n")
#head(html_text(dt))
library(tidyverse)
library(tidytext)
library(quanteda)
library(dplyr)
library(rvest)
library(xml2)
library(tibble)
texto <- (html_text(dt))
texto <- str_replace_all(texto,"\\(",")")
texto <- str_replace_all(texto, "\\)","")
texto <- str_replace_all(texto,"\n","")
texto <- str_replace_all(texto,"\t","")
texto
text_df <- tibble(line=length(texto), text= texto)
text_df
write.csv(text_df,file = "text_df.csv")
library(tidytext)
text_token <- text_df %>%
unnest_tokens(word, text)
head(text_token)
#stopwords
library(quanteda)
library(stopwords)
library(tibble)
library(dplyr)
stop_w <- tibble(word = stopwords(source = "stopwords-iso", language = "pt"))
??stopwords-iso
#stopwords
library(quanteda)
library(stopwords)
library(tibble)
library(dplyr)
stop_w <- tibble(word = stop_words(source = "stopwords-iso", language = "pt"))
??stopwords()
library(tm)
stop_w <- tibble(word = stopwords(source = "stopwords-iso", language = "pt"))
library(quanteda)
library(quanteda.textplots)
knitr::opts_chunk$set(echo = TRUE,message = F,warning = F)
#stopwords
library(quanteda)
library(stopwords)
library(tibble)
library(dplyr)
library(tm)
stop_w <- tibble(word = stopwords(source = "stopwords-iso", language = "pt"))
q()
knitr::opts_chunk$set(echo = TRUE,message = F,warning = F)
# Baixando e carregando o pacote
library( rvest )
library( xml2 )
library(shiny)
library(httr)
library(httr2)
# Link para:
url <-  " https://jornaldaparaiba.com.br/ "
url
# Ler o HTML da página da web
url <- c("https://www.jornaldaparaiba.com.br")
pagina <- read_html(url)
elementos <- pagina %>%
html_nodes("div.container a")%>%
html_text2()
# Carregar o pacote rvest
library(rvest)
library(xml2)
library(shiny)
library(httr)
library(httr2)
# Ler o HTML da página da web
url <- "https://www.jornaldaparaiba.com.br"
pagina <- read_html(url)
dt <- html_nodes(pagina, "div.container a")
# Exibir o conteúdo dos elementos selecionados
cat("Conteúdo dos elementos selecionados:\n")
cat(html_text(dt), sep = "\n")
#head(html_text(dt))
library(tidyverse)
library(tidytext)
library(quanteda)
library(dplyr)
library(rvest)
library(xml2)
library(tibble)
texto <- (html_text(dt))
texto <- str_replace_all(texto,"\\(",")")
texto <- str_replace_all(texto, "\\)","")
texto <- str_replace_all(texto,"\n","")
texto <- str_replace_all(texto,"\t","")
texto
text_df <- tibble(line=length(texto), text= texto)
text_df
write.csv(text_df,file = "text_df.csv")
library(tidytext)
text_token <- text_df %>%
unnest_tokens(word, text)
head(text_token)
#stopwords
library(quanteda)
library(stopwords)
library(tibble)
library(dplyr)
library(tm)
library(tidytext)
#stop_w <- tibble(word = stopwords(source = "stopwords-iso", language = "pt"))
stop_w <- tibble(word =stopwords("pt"))
#retirar do corpus as stopwords
tidy_text <- text_token %>%
anti_join(stop_w)
text_token %>%
count(word, sort = TRUE)
library(ggplot2)
text_token %>%
count(word, sort = TRUE) %>%
mutate(word = fct_reorder(word, n)) %>%
slice(1:20) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
labs(x="")
#Transformando o data.frame em um corpus
corp <- corpus(texto)
summary(corp, 10)
#Tokens sem pontuação e sem sequência
toks <- tokens(corp, remove_punct = T, remove_numbers = T)
# Removendo as stopwords
toks_nostop <- tokens_select(toks, pattern = stopwords('pt'),
selection = 'remove')
# criando n-grams
toks_ngram <- tokens_ngrams(toks, n = 1:4)
toks_ngram
library(quanteda.textplots)
#criando uma DFM com as hashtags
dfmat_texto <- dfm(toks)
set.seed(132)
textplot_wordcloud(dfmat_texto, max_words = 100)
library(rvest)
library(xml2)
library(dplyr)
library(knitr)
dt_noticias <- html_nodes(pagina,xpath=" //h1 |//h2 |//h5")%>%
html_text2()
dt_noticias
#dt_titulos <- html_element(pagina,"section")%>%
#html_text2()
#dt_titulos
#Realizando a limpeza no texto
texto <- str_replace_all(dt_noticias,"\\(",")")
texto <- str_replace_all(dt_noticias, "\\)","")
texto <- str_replace_all(dt_noticias,"\n","")
texto <- str_replace_all(dt_noticias,"\t","")
texto
texto_df <- tibble(line=length(texto), text= texto) %>%
kable()
texto_df
# Instalar e carregar o pacote rvest
#install.packages("rvest")
library(rvest)
library(xml2)
# Definir a URL da página de notícias do G1
url <- "https://www.jornaldaparaiba.com.br/"
# Coletar os títulos e links das notícias
noticias <- read_html(url) %>%
html_nodes(xpath=" //h1 |//h2 |//h5") %>%
html_text() %>%
data.frame(Título = .) %>%
mutate(Link = read_html(url) %>%
html_element("a") %>%
html_attr("href"))
# Exibir as primeiras linhas do resultado
head(noticias)
# Instalar e carregar o pacote rvest
#install.packages("rvest")
library(rvest)
library(xml2)
library(tidytext)
# Definir a URL da página de notícias mais lidas do G1
url <- "https://www.jornaldaparaiba.com.br"
# Coletar os títulos, links, autores e datas das notícias mais lidas
noticias_lidas <- read_html(url) %>%
html_nodes(xpath=" //h1 |//h2 |//h5") %>%
html_text2() %>%
data.frame(Título = .) %>%
mutate(Link = read_html(url) %>%
html_element("a") %>%
html_attr("href"),
Autor = read_html(url) %>%
html_element(xpath = "//div[2] |//div[1] |//section |//div |/ /h4 |//a") %>%
html_text(),
Data = read_html(url) %>%
html_element(xpath = "//div[1] |//header |//div[1] |//div[1]") %>%
html_text2())
noticias_lidas <- str_replace_all(noticias_lidas,"\\(",")")
noticias_lidas <- str_replace_all(noticias_lidas, "\\)","")
noticias_lidas <- str_replace_all(noticias_lidas,"\n","")
noticias_lidas <- str_replace_all(noticias_lidas,"\t","")
# Exibir as primeiras linhas do resultado
head(noticias_lidas)
# Instalar e carregar os pacotes rvest e tidytext
#install.packages(c("rvest", "tidytext"))
library(rvest)
library(tidytext)
library(syuzhet)
# Definir a URL da página de notícias do Jornal da Paraíba
#url <- "https://www.jornaldaparaiba.com.br/"
# Coletar os títulos das notícias
noticias <- read_html(url) %>%
html_nodes(xpath= "//h1 |//h2 |//h5") %>%
html_text2()
# Transformar os títulos em um data frame para a análise de sentimentos
df_noticias <- data.frame(Título = noticias, stringsAsFactors = FALSE)
# Separar cada palavra do título em uma linha
texto_palavras <- get_tokens(texto)
head(texto_palavras)
# Carregue os pacotes
library(syuzhet)
library(RColorBrewer)
library(wordcloud)
library(tm)
head(texto_palavras)
length(texto_palavras)
oracoes_vetor <- get_sentences(noticias)
length(oracoes_vetor)
sentimentos_df <- get_nrc_sentiment(texto_palavras, lang="portuguese")
head(sentimentos_df)
summary(sentimentos_df)
barplot(
colSums(prop.table(sentimentos_df[, 1:8])),
space = 0.2,
horiz = FALSE,
las = 1,
cex.names = 0.7,
col = brewer.pal(n = 8, name = "Set3"),
main = "Análise de sentimentos",
sub = "Análise utilizando web scraping",
xlab="emoções", ylab = NULL, cex.axis = 0.5)
palavras_medo <- texto_palavras[sentimentos_df$fear > 0]
palavras_medo
palavras_medo_ordem <- sort(table(unlist(palavras_medo)), decreasing = TRUE)
head(palavras_medo_ordem, n = 10)
length(palavras_medo_ordem)
nuvem_emocoes_vetor <- c(
paste(texto_palavras[sentimentos_df$sadness> 0], collapse = " "),
paste(texto_palavras[sentimentos_df$joy > 0], collapse = " "),
paste(texto_palavras[sentimentos_df$anger > 0], collapse = " "),
paste(texto_palavras[sentimentos_df$fear > 0], collapse = " "))
nuvem_emocoes_vetor <- iconv(nuvem_emocoes_vetor, "latin1", "UTF-8")
nuvem_corpus <- Corpus(VectorSource(nuvem_emocoes_vetor))
nuvem_tdm <- TermDocumentMatrix(nuvem_corpus)
nuvem_tdm <- as.matrix(nuvem_tdm)
head(nuvem_tdm)
colnames(nuvem_tdm) <- c('tristeza', 'felicidade', 'raiva', 'confiança')
head(nuvem_tdm)
library(quanteda.textplots)
#criando uma DFM com as hashtags
dfmat_texto <- dfm(texto_palavras)
set.seed(132)
textplot_wordcloud(dfmat_texto, max_words = 100, colors = c("green", "red", "orange", "blue"))
sentimentos_valencia <- (sentimentos_df$negative * -1) + sentimentos_df$positive
sentimentos_valencia
simple_plot(sentimentos_valencia)
write.csv(sentimentos_df, file = "analise_sent_titulos.csv", row.names = texto_palavras)
knitr::opts_chunk$set(echo = TRUE,message = F,warning = F)
# Baixando e carregando o pacote
library( rvest )
library( xml2 )
library(shiny)
library(httr)
library(httr2)
# Link para:
url <-  " https://jornaldaparaiba.com.br/ "
url
# Ler o HTML da página da web
url <- c("https://www.jornaldaparaiba.com.br")
pagina <- read_html(url)
elementos <- pagina %>%
html_nodes("div.container a")%>%
html_text2()
# Carregar o pacote rvest
library(rvest)
library(xml2)
library(shiny)
library(httr)
library(httr2)
# Ler o HTML da página da web
url <- "https://www.jornaldaparaiba.com.br"
pagina <- read_html(url)
dt <- html_nodes(pagina, "div.container a")
# Exibir o conteúdo dos elementos selecionados
cat("Conteúdo dos elementos selecionados:\n")
cat(html_text(dt), sep = "\n")
#head(html_text(dt))
library(tidyverse)
library(tidytext)
library(quanteda)
library(dplyr)
library(rvest)
library(xml2)
library(tibble)
texto <- (html_text(dt))
texto <- str_replace_all(texto,"\\(",")")
texto <- str_replace_all(texto, "\\)","")
texto <- str_replace_all(texto,"\n","")
texto <- str_replace_all(texto,"\t","")
texto
text_df <- tibble(line=length(texto), text= texto)
text_df
write.csv(text_df,file = "text_df.csv")
library(tidytext)
text_token <- text_df %>%
unnest_tokens(word, text)
head(text_token)
#stopwords
library(quanteda)
library(stopwords)
library(tibble)
library(dplyr)
library(tm)
library(tidytext)
#stop_w <- tibble(word = stopwords(source = "stopwords-iso", language = "pt"))
stop_w <- tibble(word =stopwords("pt"))
#retirar do corpus as stopwords
tidy_text <- text_token %>%
anti_join(stop_w)
text_token %>%
count(word, sort = TRUE)
library(ggplot2)
text_token %>%
count(word, sort = TRUE) %>%
mutate(word = fct_reorder(word, n)) %>%
slice(1:20) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
labs(x="")
#Transformando o data.frame em um corpus
corp <- corpus(texto)
summary(corp, 10)
#Tokens sem pontuação e sem sequência
toks <- tokens(corp, remove_punct = T, remove_numbers = T)
# Removendo as stopwords
toks_nostop <- tokens_select(toks, pattern = stopwords('pt'),
selection = 'remove')
# criando n-grams
toks_ngram <- tokens_ngrams(toks, n = 1:4)
toks_ngram
library(quanteda.textplots)
#criando uma DFM com as hashtags
dfmat_texto <- dfm(toks)
set.seed(132)
textplot_wordcloud(dfmat_texto, max_words = 100)
library(rvest)
library(xml2)
library(dplyr)
library(knitr)
dt_noticias <- html_nodes(pagina,xpath=" //h1 |//h2 |//h5")%>%
html_text2()
dt_noticias
#dt_titulos <- html_element(pagina,"section")%>%
#html_text2()
#dt_titulos
#Realizando a limpeza no texto
texto <- str_replace_all(dt_noticias,"\\(",")")
texto <- str_replace_all(dt_noticias, "\\)","")
texto <- str_replace_all(dt_noticias,"\n","")
texto <- str_replace_all(dt_noticias,"\t","")
texto
texto_df <- tibble(line=length(texto), text= texto)
texto_df
# Instalar e carregar o pacote rvest
#install.packages("rvest")
library(rvest)
library(xml2)
# Definir a URL da página de notícias do G1
url <- "https://www.jornaldaparaiba.com.br/"
# Coletar os títulos e links das notícias
noticias <- read_html(url) %>%
html_nodes(xpath=" //h1 |//h2 |//h5") %>%
html_text() %>%
data.frame(Título = .) %>%
mutate(Link = read_html(url) %>%
html_element("a") %>%
html_attr("href"))
# Exibir as primeiras linhas do resultado
head(noticias)
# Instalar e carregar o pacote rvest
#install.packages("rvest")
library(rvest)
library(xml2)
library(tidytext)
# Definir a URL da página de notícias mais lidas do G1
url <- "https://www.jornaldaparaiba.com.br"
# Coletar os títulos, links, autores e datas das notícias mais lidas
noticias_lidas <- read_html(url) %>%
html_nodes(xpath=" //h1 |//h2 |//h5") %>%
html_text2() %>%
data.frame(Título = .) %>%
mutate(Link = read_html(url) %>%
html_element("a") %>%
html_attr("href"),
Autor = read_html(url) %>%
html_element(xpath = "//div[2] |//div[1] |//section |//div |/ /h4 |//a") %>%
html_text(),
Data = read_html(url) %>%
html_element(xpath = "//div[1] |//header |//div[1] |//div[1]") %>%
html_text2())
noticias_lidas <- str_replace_all(noticias_lidas,"\\(",")")
noticias_lidas <- str_replace_all(noticias_lidas, "\\)","")
noticias_lidas <- str_replace_all(noticias_lidas,"\n","")
noticias_lidas <- str_replace_all(noticias_lidas,"\t","")
# Exibir as primeiras linhas do resultado
head(noticias_lidas)
# Instalar e carregar os pacotes rvest e tidytext
#install.packages(c("rvest", "tidytext"))
library(rvest)
library(tidytext)
library(syuzhet)
# Definir a URL da página de notícias do Jornal da Paraíba
#url <- "https://www.jornaldaparaiba.com.br/"
# Coletar os títulos das notícias
noticias <- read_html(url) %>%
html_nodes(xpath= "//h1 |//h2 |//h5") %>%
html_text2()
# Transformar os títulos em um data frame para a análise de sentimentos
df_noticias <- data.frame(Título = noticias, stringsAsFactors = FALSE)
# Separar cada palavra do título em uma linha
texto_palavras <- get_tokens(texto)
head(texto_palavras)
# Carregue os pacotes
library(syuzhet)
library(RColorBrewer)
library(wordcloud)
library(tm)
head(texto_palavras)
length(texto_palavras)
oracoes_vetor <- get_sentences(noticias)
length(oracoes_vetor)
sentimentos_df <- get_nrc_sentiment(texto_palavras, lang="portuguese")
head(sentimentos_df)
summary(sentimentos_df)
barplot(
colSums(prop.table(sentimentos_df[, 1:8])),
space = 0.2,
horiz = FALSE,
las = 1,
cex.names = 0.7,
col = brewer.pal(n = 8, name = "Set3"),
main = "Análise de sentimentos",
sub = "Análise utilizando web scraping",
xlab="emoções", ylab = NULL, cex.axis = 0.5)
palavras_medo <- texto_palavras[sentimentos_df$fear > 0]
palavras_medo
palavras_medo_ordem <- sort(table(unlist(palavras_medo)), decreasing = TRUE)
head(palavras_medo_ordem, n = 10)
length(palavras_medo_ordem)
nuvem_emocoes_vetor <- c(
paste(texto_palavras[sentimentos_df$sadness> 0], collapse = " "),
paste(texto_palavras[sentimentos_df$joy > 0], collapse = " "),
paste(texto_palavras[sentimentos_df$anger > 0], collapse = " "),
paste(texto_palavras[sentimentos_df$fear > 0], collapse = " "))
nuvem_emocoes_vetor <- iconv(nuvem_emocoes_vetor, "latin1", "UTF-8")
nuvem_corpus <- Corpus(VectorSource(nuvem_emocoes_vetor))
nuvem_tdm <- TermDocumentMatrix(nuvem_corpus)
nuvem_tdm <- as.matrix(nuvem_tdm)
head(nuvem_tdm)
colnames(nuvem_tdm) <- c('tristeza', 'felicidade', 'raiva', 'confiança')
head(nuvem_tdm)
library(quanteda.textplots)
#criando uma DFM com as hashtags
dfmat_texto <- dfm(texto_palavras)
set.seed(132)
textplot_wordcloud(dfmat_texto, max_words = 100, colors = c("green", "red", "orange", "blue"))
sentimentos_valencia <- (sentimentos_df$negative * -1) + sentimentos_df$positive
sentimentos_valencia
simple_plot(sentimentos_valencia)
write.csv(sentimentos_df, file = "analise_sent_titulos.csv", row.names = texto_palavras)
q()
